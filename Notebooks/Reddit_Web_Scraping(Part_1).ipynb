{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit_Web_Scraping(Part-1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1UAeGTwt9-8K_vibBybeQqMeg0D6Q0fnN",
      "authorship_tag": "ABX9TyNI9f/YYed9gtGeBmddNCWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikV-JS/Reddit-Flair-Detector/blob/master/Notebooks/Reddit_Web_Scraping(Part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG9AMe0cZ15Q",
        "colab_type": "text"
      },
      "source": [
        "The first part of the assignment deals with the extraction of required data from Subreddit India posts. To achieve this, the approach is to use the Reddit API feature for web scraping. Firstly, a Reddit API account with client id, secret key and name is set up on the Reddit account so that we can connect to Reddit API for webscraping of posts. Initially, for the extraction of recent data, the PRAW (Python Wrapper for the Reddit API) module is used. PRAW module simplifies the Reddit API calls into simple function calls enabling quick and efficient web scraping. So, in the following code block we start by installing the required dependencies for the Part I notebook. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvQttYt727E9",
        "colab_type": "code",
        "outputId": "3aa60e66-c2fb-4299-dfeb-ee62caae0453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -q praw pandas numpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 143kB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 38.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jTIjbn4QT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required modules for the notebook\n",
        "import praw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad7LrzcYdTgP",
        "colab_type": "text"
      },
      "source": [
        "On Reddit, a number of submissions pertaining to different categories are submitted everyday. For filtering purposes, Reddit tags its posts into various categories called flairs. Few flair categories remain permanent on Reddit, where as few flair categories are dynamic and change according to the current trend worldwide. For example, 'Coronavirus' flair posts show up only after Feb '20. The line of code below, defines a list of flair names on the Reddit Website as of April 6th, 2020. There are a total of 12 flair categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5b2AcC-Ugb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flairs = [\"Scheduled\", \"Politics\", \"Photography\", \"Policy/Economy\", \"AskIndia\", \"Sports\", \"Non-Political\", \"Science/Technology\", \"Food\", \"Business/Finance\", \"Coronavirus\", \"CAA-NRC-NPR\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBv3335qf48r",
        "colab_type": "text"
      },
      "source": [
        "The approach for Reddit Flair Detection will be based on only Subreddit post's body and title but not the top comments on the subreddit post. Intuitively, there might be a improvement in performance of classifier if comments are also used but my goal was to build a classifier based on only body and title. The main reason behind this is having the non popular reddit posts in mind. Non popular reddit posts in generic categories and newly posted reddit posts tend to have no comments. So, if comments are considered as a feature the mean sequence length tends to move towards the larger side and thereby might cause hinderance of model performance on small and concise posts or posts with just title and link or the above type of posts with no comments. Also non inclusion of comments provides an opportunity to study the model's inference capabilities in classifying vague and short posts on Reddit. Accordingly, the following code block uses PRAW to make Reddit API calls to web scrape required data from Subreddit India posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsdHlHvBehua",
        "colab_type": "code",
        "outputId": "812f7ddc-e7c6-4c45-e864-3078b6b60a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "reddit = praw.Reddit(client_id='EPeQ4_tZaSnieQ', client_secret=\"o8wiYMDri2RMiF1um14L1rGHXEs\", user_agent='Reddit WebScraping')\n",
        "\n",
        "subreddit = reddit.subreddit('india')\n",
        "topics_dict = {\"flair\":[], \"title\":[], \"url\":[], \"comms_num\": [], \"body\":[], \"author\":[]}\n",
        "\n",
        "for flair in flairs:\n",
        "  \n",
        "  get_subreddits = subreddit.search(flair, limit=None)\n",
        "  \n",
        "  for submission in get_subreddits:\n",
        "    \n",
        "    topics_dict[\"flair\"].append(flair)\n",
        "    topics_dict[\"title\"].append(submission.title)\n",
        "    topics_dict[\"url\"].append(submission.url)\n",
        "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
        "    topics_dict[\"body\"].append(submission.selftext)\n",
        "    topics_dict[\"author\"].append(submission.author)\n",
        "\n",
        "new_data = pd.DataFrame(topics_dict)\n",
        "new_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2663, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK6PsCFRlwOZ",
        "colab_type": "text"
      },
      "source": [
        "The above code block was successful in extracting web data but there were limitations in the size of the dataset. Reddit API limits the size of a query search to 1,000 posts. I had a figure of 6,000 cases as apt for the dataset so this limitation was not a problem as I required 500 posts from each flair category. But due to limitations on Reddit's end I ended up with 2663 cases with almost all categories having cases near 250 capped. After some research, work arounds for this in praw such as cloudsearch and submissions have been deprecated. So another alternative for this was to use the archived Reddit posts on Google Cloud Platform Big Query. The following code block denotes the SQL search code used to obtain required data of 10 flair categories from the latest available months."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Cpbz6ftqjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code block will not work, please skip it\n",
        "Big Query SQL Search Code:\n",
        "SELECT * except (domain, subreddit, author_flair_css_class, link_flair_css_class, author_flair_text,\n",
        "                 from_kind, saved, hide_score, archived, from_id, name, quarantine, distinguished, stickied,\n",
        "                 thumbnail, is_self, retrieved_on, gilded, subreddit_id)\n",
        "FROM `fh-bigquery.reddit_posts.2019_06`\n",
        "WHERE subreddit = \"india\" and link_flair_text in (\"Scheduled\", \"Politics\", \"Photography\", \"Policy/Economy\", \"AskIndia\", \"Sports\", \"Non-Political\", \"Science/Technology\", \"Food\", \"Business/Finance\", \"Coronavirus\", \"CAA-NRC-NPR\")\n",
        "LIMIT 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwgfTBy8bQEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the Csv data obtained from the Big Query SQL search code\n",
        "data_08 = pd.read_csv('/content/data_2019_08.csv') #path to the 2019_08 Big Query Data needs to be provided\n",
        "data_07 = pd.read_csv('/content/data_2019_07.csv') #path to the 2019_07 Big Query Data needs to be provided\n",
        "data_06 = pd.read_csv('/content/data_2019_06.csv') #path to the 2019_06 Big Query Data needs to be provided"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMmSjqdgb2MY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code block was used to obtain a stastical inference of the number of flairs in each of the 4 datasets.\n",
        "length = np.zeros((12,1))\n",
        "length_19_08 = np.zeros((12,1))\n",
        "length_19_07 = np.zeros((12,1))\n",
        "length_19_06 = np.zeros((12,1))\n",
        "for i in range(0,12):\n",
        "  flair_data_08 = data_08[data_08['link_flair_text'] == flairs[i]];\n",
        "  length_19_08[i] = flair_data_08.shape[0];\n",
        "for i in range(0,12):\n",
        "  new_flair_data = new_data[new_data['flair'] == flairs[i]];\n",
        "  length[i] = new_flair_data.shape[0];\n",
        "for i in range(0,12):\n",
        "  flair_data_07 = data_07[data_07['link_flair_text'] == flairs[i]];\n",
        "  length_19_07[i] = flair_data_07.shape[0];\n",
        "for i in range(0,12):\n",
        "  flair_data_06 = data_06[data_06['link_flair_text'] == flairs[i]];\n",
        "  length_19_06[i] = flair_data_06.shape[0];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNHjkhNTyIVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The following code block was used to merge the 4 datasets and obtain the final dataset with only three fields i.e flair, body, text.\n",
        "o_data = [data_06, data_07, data_08]\n",
        "o_data = pd.concat(o_data) # Create a Pandas DataFrame with all the Big Query Data\n",
        "data = o_data[['link_flair_text','title','selftext']] # Selecting only the required fields from o_data\n",
        "data = data.rename(columns={'link_flair_text': 'flair', 'selftext': 'body'}) # Renaming the Reddit API properties to appropriate column names\n",
        "n_data = new_data[['flair','title','body']] # Selecting only the required fields from new_data obtained from PRAW\n",
        "data = [data,n_data] \n",
        "data = pd.concat(data) # Creating Final Pandas DataFrame\n",
        "data.to_csv('data.csv') # Saving the Final Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cmak4snD5TS",
        "colab_type": "text"
      },
      "source": [
        "It is always a good practice to use random sampling when refining the size of a dataset as it terminates bias generated from sequential pickup of data. The following code block uses random sampling so each time the code runs the data produced changes. A total of 6,000 cases are sampled from around 22,000 cases in the original data file. After a brief analysis of number of each flair cases in the original data file, it is evident that Poltics, AskIndia, Non-Political have a bigger distribution and also Coronavirus and CAA-NRC-NPR have less number of cases due to Reddit API limitations. So, a total size of 6,000 cases was taken and 7 of the flairs are of size 500 each whereas the size of Coronavirus and CAA-NRC-NPR category was capped off to 250 and 109 respectivley and size of Poltics, AskIndia, Non-Political cases was 714, 714, 713 respectively. After a preliminary analysis and study of the obtained final dataset I feel that the short number of cases for Coronavirus and CAA-NRC-NPR shouldn't be a difficulty for the model as the title of submission mostly gives away the flair category in these cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLxT9sxx0xcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/data.csv')\n",
        "final_data = pd.DataFrame\n",
        "for i in range(0,12):\n",
        "  if i == 0:\n",
        "    flair_data = data[data['flair'] == flairs[i]];\n",
        "    f_data = flair_data.sample(n=500);\n",
        "    final_data = f_data;\n",
        "  elif i == 1 or i == 4:\n",
        "    flair_data = data[data['flair'] == flairs[i]];\n",
        "    f_data = flair_data.sample(n=714);\n",
        "    final_data = [final_data,f_data];\n",
        "    final_data = pd.concat(final_data)\n",
        "  elif i == 6:\n",
        "    flair_data = data[data['flair'] == flairs[i]];\n",
        "    f_data = flair_data.sample(n=713);\n",
        "    final_data = [final_data,f_data];\n",
        "    final_data = pd.concat(final_data)\n",
        "  else:\n",
        "    flair_data = data[data['flair'] == flairs[i]];\n",
        "    if flair_data.shape[0] < 500:\n",
        "      n = flair_data.shape[0];\n",
        "    else:\n",
        "      n = 500;\n",
        "    f_data = flair_data.sample(n);\n",
        "    final_data = [final_data,f_data];\n",
        "    final_data = pd.concat(final_data)\n",
        "final_data = final_data[['flair','title','body']]\n",
        "final_data.to_csv('Final_Dataset.csv', index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZguglTptszY7",
        "colab_type": "text"
      },
      "source": [
        "Verification of size of final obtained dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kQ2G9vN_74Y",
        "colab_type": "code",
        "outputId": "71c58e17-0cd5-49a1-f610-16afbc96c131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "final_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgY-X4rJEX2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flair_length = np.zeros((12,1));\n",
        "for i in range(0,12):\n",
        "  flair = final_data[final_data['flair'] == flairs[i]];\n",
        "  flair_length[i] = flair.shape[0];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDf4f7hoE_g6",
        "colab_type": "code",
        "outputId": "7524a79c-e3cc-4af3-fee4-1a2dab855c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "flair_length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[500.],\n",
              "       [714.],\n",
              "       [500.],\n",
              "       [500.],\n",
              "       [714.],\n",
              "       [500.],\n",
              "       [713.],\n",
              "       [500.],\n",
              "       [500.],\n",
              "       [500.],\n",
              "       [250.],\n",
              "       [109.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}